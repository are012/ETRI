{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e6f915",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = \"A Dog Run back corner near spare bedrooms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45caf13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy 사용\n",
    "\n",
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(en_text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(en_text)]\n",
    "\n",
    "print(tokenize(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234ab150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK 사용\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ddef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기로 분리\n",
    "\n",
    "print(en_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 띄어쓰기 토큰화\n",
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\"\n",
    "print(kor_text.split())\n",
    "\n",
    "# 조사가 붙어 있어서 제거 안하면 전부 다른 단어로 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 토큰화\n",
    "\n",
    "!pip install konlpy\n",
    "!pip install mecab-python\n",
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae10f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 토큰화\n",
    "\n",
    "print(list(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합(Vocabulary) 생성\n",
    "\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "data = pd.read_table('ratings.txt') # 데이터프레임에 저장\n",
    "data[:10]\n",
    "\n",
    "print('전체 샘플의 수 : {}'.format(len(data)))\n",
    "\n",
    "sample_data = data[:100]\n",
    "\n",
    "sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "sample_data[:10]\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "tokenizer = Mecab()\n",
    "tokenized=[]\n",
    "for sentence in sample_data['document']:\n",
    "    temp = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
    "    tokenized.append(temp)\n",
    "    \n",
    "print(tokenized[:10])\n",
    "\n",
    "vocab = FreqDist(np.hstack(tokenized))\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "\n",
    "vocab['재밌']\n",
    "\n",
    "vocab_size = 500\n",
    "# 상위 vocab_size개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49239ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어에 고유한 정수 부여\n",
    "\n",
    "word_to_index = {word[0]:index+1 for index, word in enumerate(vocab)}\n",
    "word_to_index['pad'] = 1\n",
    "word_to_index\n",
    "\n",
    "encoded = []\n",
    "for line in tokenized: # 입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line: # 문장에서 단어를 하나씩 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w]) # 글자에 해당되는 정수로 변환\n",
    "        except KeyError: # 단어 집합에 없는 단어는 unk로 대체\n",
    "            temp.append(word_to_index['unk']) # unk의 인덱스로 변환\n",
    "    encoded.append(temp)\n",
    "    \n",
    "print(encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92fd25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이가 다른 문장들을 모두 동일한 길이로 바꿔주는 패딩\n",
    "\n",
    "max_len = max(len(l) for l in encoded)\n",
    "print('리뷰의 최대 길이 : %d' % max_len)\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))\n",
    "plt.hist([len(s) for s in encoded], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "for line in encoded:\n",
    "    if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
    "        line += [word_to_index['pad']] * (max_len - len(line)) # 나머지는 전부 pad로 채움\n",
    "        \n",
    "print('리뷰의 최대 길이 : %d' % max(len(l) for l in encoded))\n",
    "print('리뷰의 최소 길이 : %d' % min(len(l) for l in encoded))\n",
    "print('리뷰의 평균 길이 : %f' % (sum(map(len, encoded))/len(encoded)))\n",
    "\n",
    "print(encoded[:3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
