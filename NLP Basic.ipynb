{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e6f915",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_text = \"A Dog Run back corner near spare bedrooms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45caf13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy 사용\n",
    "\n",
    "import spacy\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenize(en_text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(en_text)]\n",
    "\n",
    "print(tokenize(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234ab150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK 사용\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ddef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기로 분리\n",
    "\n",
    "print(en_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 띄어쓰기 토큰화\n",
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\"\n",
    "print(kor_text.split())\n",
    "\n",
    "# 조사가 붙어 있어서 제거 안하면 전부 다른 단어로 인식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c091ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 토큰화\n",
    "\n",
    "!pip install konlpy\n",
    "!pip install mecab-python\n",
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer = Mecab()\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a957b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 토큰화\n",
    "\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "data = pd.read_table('ratings.txt') # 데이터프레임에 저장\n",
    "data[:10]\n",
    "\n",
    "sample_data = data[:100]\n",
    "\n",
    "sample_data['document'] = sample_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\", regex=True)\n",
    "\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "sample_data[:10]\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords=['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "tokenizer = Mecab()\n",
    "tokenized=[]\n",
    "for sentence in sample_data['document']:\n",
    "    temp = tokenizer.morphs(sentence) # 토큰화\n",
    "    temp = [word for word in temp if not word in stopwords] # 불용어 제거\n",
    "    tokenized.append(temp)\n",
    "    \n",
    "print(tokenized[:10])\n",
    "\n",
    "vocab = FreqDist(np.hstack(tokenized))\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))\n",
    "\n",
    "vocab['재밌']\n",
    "\n",
    "vocab_size = 500\n",
    "# 상위 vocab_size개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "print('단어 집합의 크기 : {}'.format(len(vocab)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
