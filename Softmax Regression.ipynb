{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbc818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용 함수 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 로우 레벨\n",
    "# 첫번째 수식\n",
    "# (y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()\n",
    "# 두번째 수식\n",
    "# (y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()\n",
    "\n",
    "\n",
    "# 하이 레벨\n",
    "# 첫 번째 수식\n",
    "# F.nll_loss(F.log_softmax(z, dim=1), y)\n",
    "# 두 번째 수식\n",
    "# F.cross_entropy(z, y) // 비용 함수에 소프트맥스 함수까지 포함\n",
    "# 세 번째 수식\n",
    "# nn.CrossEntropyLoss() 클래스 사용\n",
    "# cost = nn.CrossEntropyLoss()(z, y)\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 1단계: 클래스로 객체 생성\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 2단계: 생성된 객체 사용\n",
    "loss = criterion(z, y)\n",
    "# 같은 객체로 여러 번 계산 가능\n",
    "loss1 = criterion(z, y)           # 첫 번째 계산\n",
    "loss2 = criterion(z, y)           # 두 번째 계산\n",
    "\n",
    "# 새로운 데이터가 있다면\n",
    "z2 = torch.rand(3, 5, requires_grad=True)\n",
    "y2 = torch.randint(5, (3,)).long()\n",
    "loss3 = criterion(z2, y2)         # 새 데이터로 계산\n",
    "\n",
    "\n",
    "# 같은 객체로 여러 번 계산 가능\n",
    "loss1 = criterion(z, y)           # 첫 번째 계산\n",
    "loss2 = criterion(z, y)           # 두 번째 계산\n",
    "\n",
    "# 새로운 데이터가 있다면\n",
    "z2 = torch.rand(3, 5, requires_grad=True)\n",
    "y2 = torch.randint(5, (3,)).long()\n",
    "loss3 = criterion(z2, y2)         # 새 데이터로 계산\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 평균 대신 합계를 구하는 설정\n",
    "# 손실함수 객체를 한 번만 생성. 이제 호출할때는 무조건 criterion으로만 호출함.\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# 같은 객체로 여러 번 계산 가능\n",
    "loss1 = criterion(z, y)           # 첫 번째 계산\n",
    "loss2 = criterion(z, y)           # 두 번째 계산\n",
    "\n",
    "# 새로운 데이터가 있다면\n",
    "z2 = torch.rand(3, 5, requires_grad=True)\n",
    "y2 = torch.randint(5, (3,)).long()\n",
    "loss3 = criterion(z2, y2)         # 새 데이터로 계산\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 함수 방식에서는 매번 설정을 반복해야 함\n",
    "loss_sum1 = F.cross_entropy(z, y, reduction='sum')\n",
    "loss_sum2 = F.cross_entropy(z2, y2, reduction='sum')  # 설정 반복\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0e2643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로우-레벨 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "y_one_hot = torch.zeros(8, 3)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "\n",
    "W = torch.randn((4, 3), requires_grad=True)\n",
    "b = torch.randn((1, 3), requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "    \n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e037bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이-레벨 구현\n",
    "\n",
    "W = torch.zeros((4, 3), requires_grad=True)\n",
    "b = torch.zeros((1, 3), requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Module 사용\n",
    "\n",
    "# 모델을 선언 및 초기화. 4개의 특성을 가지고 3개의 클래스로 분류. input_dim=4, output_dim=3.\n",
    "model = nn.Linear(4, 3)\n",
    "\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0438d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 사용\n",
    "\n",
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3) # Output이 3!\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = SoftmaxClassifierModel\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST 데이터 분류\n",
    "\n",
    "import torch\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
    "print(\"다음 기기로 학습합니다:\", device)\n",
    "\n",
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)\n",
    "\n",
    "# hyperparameters\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# MNIST dataset\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "# dataset loader\n",
    "data_loader = DataLoader(dataset=mnist_train,\n",
    "                                          batch_size=batch_size, # 배치 크기는 100\n",
    "                                          shuffle=True,\n",
    "                                          drop_last=True)\n",
    "\n",
    "# MNIST data image of shape 28 * 28 = 784\n",
    "linear = nn.Linear(784, 10, bias=True).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device) # 비용 함수 객체\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.1) # 최적화 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f347588",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(training_epochs): # 앞서 training_epochs의 값은 15로 지정함.\n",
    "    avg_cost = 0\n",
    "    total_batch = len(data_loader)\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        # 배치 크기가 100이므로 아래의 연산에서 X는 (100, 784)의 텐서가 된다.\n",
    "        X = X.view(-1, 28 * 28).to(device)\n",
    "        # 레이블은 원-핫 인코딩이 된 상태가 아니라 0 ~ 9의 정수.\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = linear(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 사용하여 모델을 테스트한다.\n",
    "with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다.\n",
    "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = linear(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n",
    "\n",
    "    # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다\n",
    "    r = random.randint(0, len(mnist_test) - 1)\n",
    "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
    "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
    "\n",
    "    print('Label: ', Y_single_data.item())\n",
    "    single_prediction = linear(X_single_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "\n",
    "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
