{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d653e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬으로 구현\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
    "input_size = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.\n",
    "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
    "\n",
    "inputs = np.random.random((timesteps, input_size)) # 입력에 해당되는 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화\n",
    "# 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬.\n",
    "\n",
    "Wx = np.random.random((hidden_size, input_size))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
    "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
    "b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias).\n",
    "\n",
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.\n",
    "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias)\n",
    "  total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
    "  print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
    "  hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0) \n",
    "# 출력 시 값을 깔끔하게 해준다.\n",
    "\n",
    "print(total_hidden_states) # (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9779741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치로 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)\n",
    "\n",
    "# Deep RNN\n",
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)\n",
    "\n",
    "# Bidirectional RNN\n",
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional=True)\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)\n",
    "\n",
    "# LSTM\n",
    "cell = nn.LSTM(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
    "\n",
    "# GRU\n",
    "cell = nn.GRU(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 단위\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))\n",
    "\n",
    "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)\n",
    "\n",
    "index_to_char ={}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)\n",
    "\n",
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "\n",
    "# 배치 차원 추가\n",
    "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)\n",
    "\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "\n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net(input_size, hidden_size, output_size)\n",
    "\n",
    "outputs = net(X)\n",
    "print(outputs.shape)\n",
    "\n",
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환\n",
    "\n",
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "\n",
    "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 많은 데이터로 학습한 문자 단위\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
    "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
    "\n",
    "print(char_dic)\n",
    "\n",
    "dic_size = len(char_dic)\n",
    "print('문자 집합의 크기 : {}'.format(dic_size))\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10 # 임의의 숫자 지정\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1:i + sequence_length + 1]\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str]) # 문자 -> 정수 인코딩\n",
    "    y_data.append([char_dic[c] for c in y_str]) # 문자 -> 정수 인코딩\n",
    "\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net(dic_size, hidden_size, dic_size)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "outputs = net(X)\n",
    "print(outputs.shape)\n",
    "\n",
    "print(outputs.view(-1, dic_size).shape)\n",
    "print(Y.view(-1).shape)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    results = outputs.argmax(dim=2) # 최종 예측값\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[c] for c in result])\n",
    "        else: # 그 다음부터는 마지막 문자 하나씩만 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "            \n",
    "    print(predict_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
