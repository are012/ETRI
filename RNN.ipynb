{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d653e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬으로 구현\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
    "input_size = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.\n",
    "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
    "\n",
    "inputs = np.random.random((timesteps, input_size)) # 입력에 해당되는 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화\n",
    "# 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬.\n",
    "\n",
    "Wx = np.random.random((hidden_size, input_size))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
    "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
    "b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias).\n",
    "\n",
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.\n",
    "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias)\n",
    "  total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
    "  print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
    "  hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0) \n",
    "# 출력 시 값을 깔끔하게 해준다.\n",
    "\n",
    "print(total_hidden_states) # (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9779741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([1, 1, 8])\n",
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n",
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# 파이토치로 구현\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)\n",
    "\n",
    "# Deep RNN\n",
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)\n",
    "\n",
    "# Bidirectional RNN\n",
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional=True)\n",
    "outputs, _status = cell(inputs)\n",
    "print(outputs.shape)\n",
    "print(_status.shape)\n",
    "\n",
    "# LSTM\n",
    "cell = nn.LSTM(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)\n",
    "\n",
    "# GRU\n",
    "cell = nn.GRU(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf3c6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n",
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n",
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n",
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n",
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n",
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n",
      "torch.Size([1, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n",
      "0 loss:  1.7013962268829346 prediction:  [[3 1 1 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  laap!\n",
      "1 loss:  1.4020740985870361 prediction:  [[4 3 3 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pllp!\n",
      "2 loss:  1.2104556560516357 prediction:  [[4 4 3 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pplp!\n",
      "3 loss:  1.0410974025726318 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "4 loss:  0.8741825222969055 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "5 loss:  0.7036722898483276 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "6 loss:  0.5508555173873901 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "7 loss:  0.4178709089756012 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.3062327206134796 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.218479722738266 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.15349450707435608 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.107274129986763 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.07588275521993637 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.055345118045806885 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.04170268401503563 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.03204624727368355 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.024897992610931396 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.01957108825445175 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.015608960762619972 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.012651520781219006 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.010424287989735603 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.008725592866539955 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.007410692982375622 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.006377037614583969 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.0055521209724247456 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.004884467925876379 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.004337177146226168 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.003883323399350047 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.003503096755594015 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.0031817189883440733 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.0029078975785523653 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.0026728117372840643 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.0024697906337678432 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.00229343818500638 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.002139442600309849 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.00200439291074872 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.001885444507934153 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.0017801783978939056 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.0016868397360667586 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0016036720480769873 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0015293702017515898 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.001462794840335846 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0014029479352757335 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0013490223791450262 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0013002812629565597 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0012561546172946692 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.0012160241603851318 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.0011794858146458864 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.0011461597168818116 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0011156410910189152 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.001087644835934043 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.001061909133568406 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.0010381487663835287 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.0010161732789129019 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.0009958638111129403 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.0009769824100658298 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0009594340808689594 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.0009430756908841431 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.0009276933851651847 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.0009133346611633897 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0008999280398711562 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.0008871880127117038 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0008751622517593205 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0008638506988063455 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.0008531103958375752 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.000842965324409306 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0008332249708473682 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0008239845628850162 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.0008151727961376309 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0008066704613156617 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0007985966512933373 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.0007907847757451236 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.0007833063718862832 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.000776065862737596 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.0007690872298553586 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.00076234684092924 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.0007558207144029438 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.0007495087338611484 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.0007433397695422173 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0007373850094154477 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0007315493421629071 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0007259041303768754 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.0007203778950497508 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0007149468874558806 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.0007096827612258494 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.0007045612437650561 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0006994874565862119 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0006945565692149103 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0006897208513692021 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0006849566707387567 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0006803114665672183 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.00067566626239568 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0006712115136906505 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.0006667806301265955 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.0006624211091548204 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.0006581568741239607 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.0006539402529597282 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.000649771245662123 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.000645649794023484 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.0006416713586077094 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "# 문자 단위\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))\n",
    "\n",
    "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1\n",
    "\n",
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)\n",
    "\n",
    "index_to_char ={}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)\n",
    "\n",
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "\n",
    "# 배치 차원 추가\n",
    "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)\n",
    "\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "\n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net(input_size, hidden_size, output_size)\n",
    "\n",
    "outputs = net(X)\n",
    "print(outputs.shape)\n",
    "\n",
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환\n",
    "\n",
    "print(Y.shape)\n",
    "print(Y.view(-1).shape)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "\n",
    "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a1491f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 0, 'g': 1, '.': 2, 's': 3, \"'\": 4, 'y': 5, ',': 6, 'n': 7, 'c': 8, 'i': 9, 'l': 10, 'm': 11, 'o': 12, 'd': 13, 'a': 14, 'b': 15, 'p': 16, 'k': 17, 't': 18, ' ': 19, 'w': 20, 'u': 21, 'e': 22, 'h': 23, 'r': 24}\n",
      "문자 집합의 크기 : 25\n",
      "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
      "레이블의 크기 : torch.Size([170, 10])\n",
      "torch.Size([170, 10, 25])\n",
      "torch.Size([1700, 25])\n",
      "torch.Size([1700])\n",
      "yeieeyieyyeieyiiyyeiiyi,lyeiieeyyeieyyyiynieyeyiyieyyyeeyyieyinsiiyneieeyyiyyyideyyeiye,yeyyeyyiiey,yyiyyyieeyyyiiyeiyyeyyyieyyesieyyiieyiiyyyiiyyieyyieyyiyy,isiiyyyyeyieiieyyi,yy\n",
      "eddedededdededddededdddeeddddddddeddddddeddddddeddeddddedddedddddeddddeddddddddddddeddddddddeedddedddddddddeddddddeddddeedddedddedeedddeddeddddddddeedddddeddddeddndddedddddeeddedn\n",
      "t  o   o    o ooe e o ooeo   o  e e  o eo eo   e oo e oe  oo oo ee  oeo   oo   o  e e oooe ooe   o eo o    o  o  oe e ooe  oo   e oe   o oe  e o  ooe oo  e  ooo     oo oo ooe ooo \n",
      "t to sto lt h dttle ttla lt dtldt d lh dtldtl    lh dt   tth dt tt t do   lsl ta dt dtlst tt  s t  dt dtl ta ls dtt talhe tth st th  tth dt d lo dt   t    atltt   ttth tnldt std  \n",
      "t ta  ta dt h  ut a at le ,  h  t dt t  t  a  le th  t  atth dt le t  o   tnt  o dt dt dt ltt t th dt dt  to  t  ut kt he  t   t the  to dt  e a  t  bt     tttt  at t  th th  tt  \n",
      "t to   an  rh aro   anterh  a n tharat tt ata  art    h a th atn  at arn  anm ro  t erhryn ther th kt ara ao  t trh a nhe  th  t ther  h aero a n ther ra a  tat  n  tt tn rhe tro \n",
      "t ton  anm ahntothe antoeth to '  a um ao  o    at n   e  thethn   h aon  an  aon   antetn the  thnto ant ton   aot arnhe  thethethe  theto ' to  the      at ah     uh tn the   on\n",
      "p tonmtanm ao kutme anteeth ton't aoum to do   enthes  e  thewo ee t won  anm aon't astkin doem tosts anm won , uut aanhe  thet etoe  tonwon' to  themtns est uh  t  thean toe tort\n",
      "p tonmtans wo dutme anteet, ao 't ao m to ter  eath e  e  to bo le t wor  ans wo 't assign toer tosss ans tor s dut washe  to st toe  to ten' to  toemt s e sttf  ns thean toe torn\n",
      "p tormtan' wosbumml aotiif, aod't aoum to targleath esher toubo le t worglans wor't ansigndtoer tosos ans taru, dus washer toash toer to ben' tor toemtos e sitf  nssphean toertern\n",
      "p tonm and wo bumle anteip, do 't aodm do tem lenth enhe  to bo le t worl and uod't ansigndthem tosks and uord, dut uothem to chetoem to ben' do  toemtns ensitfm ns py an themtern\n",
      "p tonm and wo butle asth p, do 't aodm ds bem lenth enhe  to bo le t worl and won't assigndthem tosks and dor , dut u them to chethem to ben' fo  toemtos ens tp  ns py an themtern\n",
      "p torl and wo buile aothip, con't aoum as beopleath echer to bo lect worl and dondt aosigndthem toshs and dar , aut aather torchethem to ben' for toemtos essiwp  ns py of therthrf\n",
      "p tar tand wo buile aoahip, con't aoum up aeopleath esher to bo lect worg and dondt assigndthem toshs and darkt dut dather torchethem to bhn' for toemtos essiim  ns py of therthaf\n",
      "p torl ant to build anship, don't arum dp peoplesto esher to co lect worg and dondt dssigndthem tosks and dark, dut dather torch them to bon' for toemsos essifm  nsigy of the siof\n",
      "p tork ant to duild tnship, don't srum up peoplesto ether to co lect worl and don't dssigndthem tosks and dork, dut dather toach them to ben' forktoemsasless fmmensity of theme of\n",
      "p torkwont to build asship, don't arum um peoplesto ether to co lect word und won't sssign them tosks and work, aut aather toach them ta ben' forktoe sadless ammendity of themseos\n",
      "p torlwant to build asshim, don't arum up peoplesto ether to co lect wood and won't assign them tosks and work, aut aather toach the  ta bong for thersadless wmmendity of the shac\n",
      "p toumwant to build a shim, don't arum up people together te coflect wood and won't assign them tosks and work, aut wather teach them ta bong for tee sndless immensity of the shac\n",
      "p toum ant to build a shim, don't drum up people toge her te coflect wood and won't assign them tosks and work, but rather teach them ta bong for the dndldss immensity of the dhos\n",
      "p toul ant to build a ship, don't drum up people together to bollect wood and won't assign them tosks and work, but rather toach them to bong for the tndless immensity of the dhos\n",
      "l toul ant to build a ship, don't arum up people together to bollect wood and won't assign them tosks and work, but rather toach them to bong for the pndless immensity of therteoc\n",
      "l toulwant to build a ship, don't arum up people together to bollect wood and don't assign them tosks and work, but rather toach them to long for the pndless immensity of thereeas\n",
      "l toulwant to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the eeas\n",
      "l tou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the eeat\n",
      "l tou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the eeat\n",
      "l tou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeat\n",
      "t tou want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeat\n",
      "t tou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeac\n",
      "t tou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeac\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeat\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeat\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the eeat\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seat\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "# 더 많은 데이터로 학습한 문자 단위\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
    "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
    "\n",
    "print(char_dic)\n",
    "\n",
    "dic_size = len(char_dic)\n",
    "print('문자 집합의 크기 : {}'.format(dic_size))\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10 # 임의의 숫자 지정\n",
    "learning_rate = 0.1\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1:i + sequence_length + 1]\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str]) # 문자 -> 정수 인코딩\n",
    "    y_data.append([char_dic[c] for c in y_str]) # 문자 -> 정수 인코딩\n",
    "\n",
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "net = Net(dic_size, hidden_size, dic_size)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "outputs = net(X)\n",
    "print(outputs.shape)\n",
    "\n",
    "print(outputs.view(-1, dic_size).shape)\n",
    "print(Y.view(-1).shape)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    results = outputs.argmax(dim=2) # 최종 예측값\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[c] for c in result])\n",
    "        else: # 그 다음부터는 마지막 문자 하나씩만 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "            \n",
    "    print(predict_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
